---
title: "FMB819: Rì„ ì´ìš©í•œ ë°ì´í„°ë¶„ì„"
subtitle: "<span style='font-size:1.5em; color:#a01616;'>Simple Linear Regression</span>"
# date: "`r Sys.Date()`"
format:
  revealjs:
    slide-number: true
    # smaller: true
    scrollable: true
    chalkboard: true
    transition: fade
    transition-speed: fast
    #incremental: true
    #lib_dir: libs
    css: [default, "../css/kubs.css", "../css/kubs-fonts.css"]
    #nature:
      # beforeInit: ["../js/ru_xaringan.js"]
      #highlightStyle: github
      #highlightLines: true
      #countIncrementalSlides: false
    #  ratio: "16:9"
    # includes:
    #   in_header: "../libs/partials/header.html"
revealjs-plugins:
  - revealjs-text-resizer
---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

library("tidyverse")
library("kableExtra")
# library("repmis")
library("gridExtra")
library("haven")
library("ggpubr")
library("huxtable")
library(countdown)
# countdown style
countdown(
  color_border              = "#d90502",
  color_text                = "black",
  color_running_background  = "#d90502",
  color_running_text        = "white",
  color_finished_background = "white",
  color_finished_text       = "#d90502",
  color_finished_border     = "#d90502"
)
```

## Today's Agenda

-   ***ë‹¨ìˆœ ì„ í˜• íšŒê·€ ëª¨í˜•(Simple Linear Regression Model)*** ë° ***ìµœì†Œì œê³±ë²•(Ordinary Least Squares, OLS)*** *ì¶”ì •* ì†Œê°œ.

-   ì‹¤ì¦ ë¶„ì„: *í•™ê¸‰ ê·œëª¨*ì™€ *í•™ìƒ ì„±ì·¨ë„*ì˜ ê´€ê³„

-   **ì¸ê³¼ì (causal)** ê´€ê³„ë¥¼ ì–´ë–»ê²Œ ë°í ìˆ˜ ìˆì„ê¹Œ?

------------------------------------------------------------------------

## í•™ê¸‰ ê·œëª¨ì™€ í•™ìƒ ì„±ì·¨ë„

-   ì–´ë–¤ ì •ì±…ì´ í•™ìƒ í•™ìŠµ ì„±ì·¨ë„ í–¥ìƒì‹œí‚¤ëŠ”ê°€?

-   í•™ê¸‰ ê·œëª¨ ì¶•ì†ŒëŠ” *ìˆ˜ì‹­ ë…„ê°„* êµìœ¡ ì •ì±… ë…¼ìŸì˜ í•µì‹¬ ì£¼ì œì˜€ìŒ.

-   [Joshua Angristì™€ Victor Lavy (1999)](https://economics.mit.edu/files/8273)ì˜ ìœ ëª…í•œ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ë¶„ì„. í•´ë‹¹ ë°ì´í„°ëŠ” [Raj Chettyì™€ Greg Bruichì˜ ê°•ì˜](https://opportunityinsights.org/course/)ì—ì„œ ì œê³µë¨.

-   1991ë…„ ì´ìŠ¤ë¼ì—˜ ìœ ëŒ€ì¸ ê³µë¦½ ì´ˆë“±í•™êµ 5í•™ë…„ìƒ(10-11ì„¸)ì˜ ì‹œí—˜ ì ìˆ˜ì™€ í•™ê¸‰/í•™êµ íŠ¹ì„± í¬í•¨.

-   êµ­ê°€ ë‹¨ìœ„ ì‹œí—˜ì„ í†µí•´ *ìˆ˜í•™* ë° (íˆë¸Œë¦¬ì–´) *ì½ê¸°* ëŠ¥ë ¥ í‰ê°€í–ˆìœ¼ë©°, ì›ì ìˆ˜ëŠ” 1-100 ì‚¬ì´ ì²™ë„ë¡œ ë³€í™˜ë¨.

------------------------------------------------------------------------

## Task 1 {background-color="#ffebf0"}

::: {style="position: absolute; top: -30px; right: 10px; font-size: 0.8em;"}
`r countdown(minutes = 7, top = 0)`
:::

1.  ë°ì´í„°ë¥¼ [ì—¬ê¸°](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1)ì—ì„œ ë¶ˆëŸ¬ì™€ `grades`ë¡œ ì €ì¥.\
    *íŒíŠ¸: `haven` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `read_dta` í•¨ìˆ˜ ì‚¬ìš©í•´ì„œ `.dta` í˜•ì‹ì˜ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ë©´ ë¨.*\
    (ì°¸ê³ : *.dta*ëŠ” [*Stata*](https://www.stata.com/)ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° íŒŒì¼ í™•ì¥ìì„.)

2.  ë°ì´í„°ì…‹ ì„¤ëª…:

    -   ê´€ì¸¡ ë‹¨ìœ„ëŠ”? ì¦‰, ê° í–‰ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€?\
    -   ì´ ëª‡ ê°œì˜ ê´€ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€?\
    -   ë°ì´í„°ì…‹ì„ í™•ì¸í•˜ê³  ì–´ë–¤ ë³€ìˆ˜ê°€ ìˆëŠ”ì§€? `avgmath`ì™€ `avgverb`ëŠ” ë­˜ ì˜ë¯¸í•˜ëŠ”ì§€?\
    -   `skimr` íŒ¨í‚¤ì§€ì˜ `skim` í•¨ìˆ˜ ì‚¬ìš©í•´ì„œ `classize`, `avgmath`, `avgverb` ë³€ìˆ˜ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ìš”ì•½ í†µê³„ í™•ì¸í•¨.\
        (*íŒíŠ¸: `dplyr`ì˜ `select` ì‚¬ìš©í•´ì„œ ë³€ìˆ˜ ì„ íƒí•œ í›„ `%>%`ë¡œ `skim()` ì ìš©í•˜ë©´ ë¨.*)

3.  í•™ê¸‰ ê·œëª¨ì™€ í•™ìƒ ì„±ì·¨ë„ ê°„ ì‹¤ì œ (ì„ í˜•) ê´€ê³„ì— ëŒ€í•´ ì–´ë–¤ ê´€ê³„ê°€ ìˆì„ ê²ƒì´ë¼ ìƒê°í•˜ëŠ”ê°€?

4.  í•™ê¸‰ ê·œëª¨ì™€ ìˆ˜í•™/ì–¸ì–´ ì ìˆ˜ ê°„ ìƒê´€ê´€ê³„ëŠ”?

```{r, echo=FALSE}
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1")
```

------------------------------------------------------------------------

## í•™ê¸‰ ê·œëª¨ì™€ í•™ìƒ ì„±ì·¨ë„: Scatter plot

::::: columns
::: {.column width="50%"}
```{r, echo=FALSE, fig.height=6}
g_math = ggplot(grades, aes(x = classize, y = avgmath)) + 
    geom_point(size = 2, alpha = 0.5) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(
      x = "Class size",
      y = "Average score",
      title = "Mathematics") +
    theme_bw(base_size = 20)
g_math
```
:::

::: {.column width="50%"}
```{r, echo=FALSE,fig.height=6}
g_verb = ggplot(grades, aes(x = classize, y = avgverb)) + 
    geom_point(size = 2, alpha = 0.5) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(x = "Class size",
         y = "Average score",
         title = "Reading") +
    theme_bw(base_size = 20)
g_verb
```
:::
:::::

-   ìƒê´€ê³„ìˆ˜ì—ì„œ ë³´ë“¯ì´ ì–´ëŠ ì •ë„ ì–‘ì˜ ê´€ê³„ ìˆìŒ. í•™ê¸‰ ê·œëª¨ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°í•´ì„œ ë” ëª…í™•í•˜ê²Œ ì‚´í´ë³´ì

------------------------------------------------------------------------

## í•™ê¸‰ ê·œëª¨ì™€ í•™ìƒ ì„±ì·¨ë„: Binned Scatter Plot

::::: columns
::: {.column width="50%"}
```{r, echo=FALSE, fig.height=6}
# Compute average scores by class size
grades_avg_cs <- grades %>%
  group_by(classize) %>%
  summarise(avgmath_cs = mean(avgmath),
            avgverb_cs = mean(avgverb))

g_math_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgmath_cs)) + 
    geom_point(size = 2) +
    xlim(0, 45) +
    ylim(0, 100) +
    labs(
      x = "Class size",
      y = "Average score",
      title = "Mathematics") +
    theme_bw(base_size = 20)
g_math_cs
```
:::

::: {.column width="50%"}
```{r, echo=FALSE,fig.height=6}
g_verb_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgverb_cs)) + 
    geom_point(size = 2) +
    xlim(0, 45) +
    ylim(0, 100) +
    labs(
      x = "Class size",
      y = "Average score",
      title = "Reading") +
    theme_bw(base_size = 20)
g_verb_cs
```
:::
:::::

------------------------------------------------------------------------

## í•™ê¸‰ ê·œëª¨ì™€ í•™ìƒ ì„±ì·¨ë„: Regression Line

í•™ê¸‰ ê·œëª¨ì™€ í•™ìƒ ì„±ì·¨ë„ì˜ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë°©ë²•: **ì‚°ì ë„ë¥¼ í†µê³¼í•˜ëŠ” ì„ (Line)**

::::: columns
::: {.column width="50%"}
```{r, echo=FALSE, fig.align='left', fig.height=4, fig.width=7}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
    geom_hline(yintercept = 65, col = "#d90502")
```
:::

::: {.column width="50%"}
```{r,echo=FALSE, fig.align='left', fig.height=4,fig.width=7}
g_math_cs +
  ylim(50, 80) +
  theme_bw(base_size = 14) +
  geom_abline(intercept = 55,slope = 0.6, col = "#d90502")
```
:::
:::::

-   ì–´ëŠ ì„ ì´ ë” ë‚˜ì€ê°€? ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ë” ë‚˜ì€ê°€?

------------------------------------------------------------------------

## ë‹¨ìˆœ ì„ í˜• íšŒê·€ (Simple Linear Regression)

ì§€ê¸ˆê¹Œì§€ ë¶„ì„ì„ ì¢€ ë” ê³µì‹ì ìœ¼ë¡œ ì •ë¦¬í•˜ìë©´

-   ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ì— ê´€ì‹¬ ìˆìŒ:

    -   ê²°ê³¼ë³€ìˆ˜ (ì¢…ì†ë³€ìˆ˜, dependent variable): *average mathematics score* $(y)$

    -   ì„¤ëª…ë³€ìˆ˜ (ë…ë¦½ë³€ìˆ˜, independent variable): *class size* $(x)$

-   ê° í•™ê¸‰ $i$ì— ëŒ€í•´ $x_i$ì™€ $y_i$ë¥¼ ê´€ì¸¡í•  ìˆ˜ ìˆìŒ.\
    ë”°ë¼ì„œ í•™ê¸‰ ê·œëª¨ì™€ í‰ê·  ìˆ˜í•™ ì ìˆ˜ì˜ *ê²°í•© ë¶„í¬(joint distribution)*ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŒ.

-   í˜„ì¬ ì´ ê´€ê³„ë¥¼ **ì„ (line)** í•˜ë‚˜ë¡œ ìš”ì•½í•˜ê³  ìˆìŒ.\
    ì ˆí¸ $b_0$ì™€ ê¸°ìš¸ê¸° $b_1$ì„ ê°–ëŠ” ì„ ì˜ ë°©ì •ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ:

$$
\widehat{y}_i = b_0 + b_1 x_i
$$

-   $\widehat{y}_i$ëŠ” *ê´€ì¸¡ê°’* $i$ì—ì„œì˜ ì˜ˆì¸¡ê°’(prediction)ì„ ì˜ë¯¸í•¨.\
    ì¦‰, ì£¼ì–´ì§„ íšŒê·€ ì§ì„ ì— ë”°ë¼ ìš°ë¦¬ê°€ $y_i$ë¥¼ ì–´ë–»ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ ë³´ì—¬ì¤Œ.

------------------------------------------------------------------------

## ì§ì„ ì˜ ë°©ì •ì‹

```{r, echo = F, fig.width = 10, fig.height = 5}
b_0 = 32
b_1 = 4.1
space = .5

base_plot <- ggplot() +
    geom_abline(slope = b_1, intercept = b_0) +
    scale_x_continuous(limits = c(-5,10), expand = c(0,0)) +
    scale_y_continuous(limits = c(0,100), expand = c(0,0)) +
    theme_bw(base_size = 16) +
    annotate(geom = "text", x = -4.3, y = 94, label = "y", parse = TRUE, size = 8, hjust = 0) +
    annotate(geom = "text", x = -4.3 + space, y = 94, label = "=", size = 8, hjust = 0) +
    annotate(geom = "text", x = -4.3 + 2*space, y = 94, label = "b[0]", color = "#DE9854", parse = TRUE, size = 8, hjust = 0) +
    annotate(geom = "text", x = -4.3 + 2*space + .6, y = 94, label = "+", size = 8, hjust = 0) +
    annotate(geom = "text", x = -4.3 + 3*space + .6, y = 94, label = "b[1]", parse = TRUE, color = "#d90502", size = 8, hjust = 0) +
    annotate(geom = "text", x = -4.3 + 4*space + .6, y = 94, label = "x", parse = TRUE, size = 8, hjust = 0)
base_plot
```

------------------------------------------------------------------------

## ì§ì„ ì˜ ë°©ì •ì‹

```{r, echo = F, fig.width = 10, fig.height = 5}
library('latex2exp')
plot_b_0 <- base_plot + 
    annotate(geom = "segment", x = 0, xend = 0, y = 0, yend = b_0, arrow = arrow(angle = 12, type = "closed"), color = "#DE9854") +
    annotate(geom = "segment", x = -5, xend = 0, y = b_0, yend = b_0, arrow = arrow(angle = 12, type = "closed", ends = "first"), color = "#DE9854") +
    scale_y_continuous(limits = c(0,100), expand = c(0,0), breaks = c(0,25,b_0,50,75,100), labels = c("0","25",parse(text = TeX("$b_0$")),"50","75","100"), minor_breaks = seq(0,100,12.5)) +
    theme(axis.text.y = element_text(color = c("grey30", "grey30", "#DE9854", "grey30", "grey30", "grey30")),
          axis.ticks.y = element_line(color = c("grey30", "grey30", "#DE9854", "grey30", "grey30", "grey30")),
          panel.grid.minor = element_line(color = c("grey92", "grey92", "grey92", "grey92", "grey92", "grey92")),
          panel.grid.major.y = element_line(color = c("grey92", "grey92", NA, "grey92", "grey92", "grey92")))
plot_b_0
```

------------------------------------------------------------------------

## ì§ì„ ì˜ ë°©ì •ì‹

```{r, echo = F, fig.width = 10, fig.height = 5}
plot_b_0 + 
    annotate(geom = "segment", x = 5, xend = 6, y = b_0+5*b_1, yend = b_0+5*b_1, arrow = arrow(angle = 12, type = "closed", length = unit(.4, "cm")), color = "#d90502") +
    annotate(geom = "text", x = 5.5, y = 49, label = "1", size = 5, color = "#d90502") +
    annotate(geom = "segment", x = 6, xend = 6, y = b_0+5*b_1, yend = b_0+6*b_1, arrow = arrow(angle = 12, type = "closed", length = unit(.2, "cm")), color = "#d90502") +
    annotate(geom = "text", x = 6.5, y = (b_0+6*b_1 + b_0+5*b_1)/2, label = "b[1]", parse = TRUE, size = 6, color = "#d90502")
```

------------------------------------------------------------------------

## Simple Linear Regression: ì”ì°¨(Residual)

-   ë§Œì•½ ëª¨ë“  ë°ì´í„°ê°€ ì§ì„  ìœ„ì— ìˆë‹¤ë©´, $\widehat{y}_i = y_i$.

```{r, echo = FALSE, fig.height = 4,fig.width=7}
x <- runif(50, min  = 0, max = 1)
y <- 1 * x

data <- data.frame(y = y,
                   x = x)

plot_ex <- data %>% ggplot(aes(x = x, y = y)) +
  geom_point() +
  xlim(0, 1) +
  ylim(0, 1) +
  labs(x = "x",
       y = "y") +
  theme_bw(base_size = 14)
plot_ex
```

------------------------------------------------------------------------

## Simple Linear Regression: ì”ì°¨(Residual)

-   ë§Œì•½ ëª¨ë“  ë°ì´í„°ê°€ ì§ì„  ìœ„ì— ìˆë‹¤ë©´, $\widehat{y}_i = y_i$.

```{r, echo = FALSE, fig.height = 4,fig.width=7}
plot_ex + geom_line(color = "#d90502")
```

------------------------------------------------------------------------

## Simple Linear Regression: ì”ì°¨(Residual)

-   ë§Œì•½ ëª¨ë“  ë°ì´í„°ê°€ ì§ì„  ìœ„ì— ìˆë‹¤ë©´, $\widehat{y}_i = y_i$.

-   ëŒ€ë¶€ë¶„ì˜ ê²½ìš° *ì¢…ì†ë³€ìˆ˜* $(y)$ëŠ” ìš°ë¦¬ê°€ ì„ íƒí•œ *ë…ë¦½ë³€ìˆ˜* $(x)$ë“¤ì— ì˜í•´ì„œë§Œ ì„¤ëª…ë˜ì§€ ì•ŠìŒ, $\widehat{y}_i \neq y_i$, ì¦‰ "ì˜¤ì°¨(error)"ê°€ í•­ìƒ ë°œìƒ. ì´ ì˜¤ì°¨ë¥¼ ì”ì°¨(residual)ë¼ ë¶€ë¦„.

-   $(x_i,y_i)$ì—ì„œì˜ ì”ì°¨ë¥¼ $e_i$ë¡œ í‘œì‹œ.

-   *ì‹¤ì œ ë°ì´í„°* $(x_i, y_i)$ëŠ” ë”°ë¼ì„œ *ì˜ˆì¸¡ê°’ + ì”*ì°¨ë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŒ

    $$
    y_i = \widehat y_i + e_i = b_0 + b_1 x_i + e_i
    $$

------------------------------------------------------------------------

## Simple Linear Regression: Graphically

```{r, echo = F, fig.width = 10, fig.height = 5}
plot_1 <- g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14)
plot_1
```

------------------------------------------------------------------------

## Simple Linear Regression: Graphically

```{r, echo = F, fig.width = 10, fig.height = 5}
plot_2 <- plot_1 +
  stat_smooth(data = grades_avg_cs, method = "lm", se = FALSE, colour = "#d90502") +
  annotate("text", x = 6.5, y = 64, label = "hat(y)", parse = TRUE, colour = "#d90502", size = 6)
plot_2
```

------------------------------------------------------------------------

## Simple Linear Regression: Graphically

```{r, echo = F, fig.width = 10, fig.height = 5}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "#d90502") +
  annotate("text", x = 6.5, y = 64, label = "hat(y)", parse = TRUE, colour = "#d90502", size = 6) +
  geom_point(data = grades_avg_cs %>% filter(classize == 17), aes(x = classize, y = avgmath_cs), color = "#d90502", size = 4) +
  annotate("text", x = 17, y = 69, label = "y[x = 17]", parse = TRUE, colour = "#d90502", size = 6)
```

------------------------------------------------------------------------

## Simple Linear Regression: Graphically

```{r, echo = F, fig.width = 10, fig.height = 5}
math_class_reg <- lm(avgmath_cs ~ classize, data = grades_avg_cs)
math_class_reg <- broom::augment(math_class_reg)

g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "#d90502") +
  annotate("text", x = 6.5, y = 64, label = "hat(y)", parse = TRUE, colour = "#d90502", size = 6) +
  geom_point(data = grades_avg_cs %>% filter(classize == 17), aes(x = classize, y = avgmath_cs), color = "#d90502", size = 4) +
  annotate("text", x = 17, y = 69, label = "y[x = 17]", parse = TRUE, colour = "#d90502", size = 6) +
  geom_segment(data = math_class_reg %>% filter(classize == 17),
               aes(xend = classize, yend = .fitted), color = "#d90502", size = 1) +
  annotate("text", x = 18, y = 65.55, label = "e[x = 17]", parse = TRUE, colour = "#d90502", size = 6)
```

------------------------------------------------------------------------

## Simple Linear Regression: Graphically

```{r, echo = F, fig.width = 10, fig.height = 5}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "#d90502") +
  annotate("text", x = 6.5, y = 64, label = "hat(y)", parse = TRUE, colour = "#d90502", size = 6) +
  geom_segment(data = math_class_reg,
               aes(xend = classize, yend = .fitted), color = "#d90502", size = 0.5)
```

------------------------------------------------------------------------

## Simple Linear Regression: Graphically

```{r, echo = F, out.width = "100%", fig.height = 4.5}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "#d90502") +
  annotate("text", x = 6.5, y = 64, label = "hat(y)", parse = TRUE, colour = "#d90502", size = 6) +
  geom_segment(data = math_class_reg,
               aes(xend = classize, yend = .fitted), color = "#d90502", size = 0.5)
```

<p style="text-align: center; font-weight: bold; font-size: 35px; color: #d90502;">

ë¬´ì—‡ì„ "ìµœì†Œí™”"í•˜ëŠ” ì§ì„ ì„ êµ¬í•´ì•¼í• ê¹Œ?</strong>

------------------------------------------------------------------------

## Ordinary Least Squares (OLS) ì¶”ì •

-   ì˜¤ì°¨(error)ì˜ ë¶€í˜¸ $(+/-)$ê°€ ì„œë¡œ ìƒì‡„. **ì œê³± ì”ì°¨(squared residuals)**ë¥¼ ê³ ë ¤ $$\forall i \in [1,N], e_i^2 = (y_i - \widehat y_i)^2 = (y_i - b_0 - b_1 x_i)^2$$

-   $\sum_{i = 1}^N e_1^2 + \dots + e_N^2$ ê°’ì´ **ìµœì†Œí™”í•˜ëŠ”** $(b_0, b_1)$ ê°’ì„ ì„ íƒ.

```{r, echo=FALSE, message=FALSE, warning=FALSE,fig.width=7,fig.height = 3.5}
g_math_cs +
    ylim(50, 80) +
    xlim(0, 50) +
    theme_bw(base_size = 14) +
    stat_smooth(method = "lm", se = FALSE, colour = "darkgreen") +
  coord_fixed(ratio = 0.65)
```

------------------------------------------------------------------------

## Ordinary Least Squares (OLS) ì¶”ì •

-   ì˜¤ì°¨(error)ì˜ ë¶€í˜¸ $(+/-)$ê°€ ì„œë¡œ ìƒì‡„. **ì œê³± ì”ì°¨(squared residuals)**ë¥¼ ê³ ë ¤ $$\forall i \in [1,N], e_i^2 = (y_i - \widehat y_i)^2 = (y_i - b_0 - b_1 x_i)^2$$

-   $\sum_{i = 1}^N e_1^2 + \dots + e_N^2$ ê°’ì´ **ìµœì†Œí™”í•˜ëŠ”** $(b_0, b_1)$ ê°’ì„ ì„ íƒ.

```{r, echo=FALSE, message=FALSE, warning=FALSE,fig.width=7,fig.height = 3.5}
g_math_cs +
    ylim(50, 80) +
    xlim(0, 50) +
    theme_bw(base_size = 14) +
    stat_smooth(method = "lm", se = FALSE, colour = "darkgreen") +
    geom_rect(data = math_class_reg,
              aes(
              xmin = classize,
              xmax = classize + abs(.resid)*0.65,
              ymin = avgmath_cs,
              ymax = avgmath_cs - .resid),
              fill = "darkgreen",
              alpha = 0.3) +
  coord_fixed(ratio = 0.65)
```

------------------------------------------------------------------------

## Ordinary Least Squares (OLS) ì¶”ì •

```{r, echo = F, out.extra = 'style="border: none;"'}
knitr::include_url("https://gustavek.shinyapps.io/reg_simple/")
```

------------------------------------------------------------------------

## Ordinary Least Squares (OLS) ì¶”ì •

```{r, echo = F, out.extra = 'style="border: none;"'}
knitr::include_url("https://gustavek.shinyapps.io/SSR_cone/")
```

------------------------------------------------------------------------

## Ordinary Least Squares (OLS) ê³„ìˆ˜ ê³µì‹

-   **OLS**: *ì”ì°¨ ì œê³±í•©(squared residuals)*ì„ ìµœì†Œí™”í•˜ëŠ” ì¶”ì • ë°©ë²•.

-   ê·¸ë ‡ë‹¤ë©´, ì ˆí¸ $b_0$ì™€ ê¸°ìš¸ê¸° $b_1$ì˜ ê³µì‹ì€?

-   í•˜ë‚˜ì˜ ë…ë¦½ë³€ìˆ˜ë§Œ ìˆëŠ” ê²½ìš°:

> ê¸°ìš¸ê¸° (Slope): $b_1^{OLS} = \frac{cov(x,y)}{var(x)}$

> ì ˆí¸ (Intercept):$b_0^{OLS} = \bar{y} - b_1\bar{x}$

-   ì”ì°¨ ì œê³±í•©ì„ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¥¼ í’€ì–´ ìœ ë„ë¨.\
    ìì„¸í•œ ìˆ˜í•™ì  ê³¼ì •ì€ [ì—¬ê¸°](https://www.youtube.com/watch?v=Hi5EJnBHFB4)ì—ì„œ í™•ì¸.

------------------------------------------------------------------------

## Ordinary Least Squares (OLS) í•´ì„

ì¢…ì† ë³€ìˆ˜ $(y)$ì™€ ë…ë¦½ ë³€ìˆ˜ $(x)$ê°€ **ìˆ«ìí˜•(numeric)**ì´ë¼ê³  ê°€ì •

> ì ˆí¸ $(b_0)$: $x = 0$ì¼ ë•Œ ì˜ˆì¸¡ëœ $y$ ê°’ $(\widehat{y})$.

> ê¸°ìš¸ê¸° $(b_1)$: $x$ê°€ í•œ ë‹¨ìœ„ ì¦ê°€í•  ë•Œ, $y$ ê°’ì´ *í‰ê· ì ìœ¼ë¡œ* ë³€í•˜ëŠ” ì •ë„

-   ë‘ ë³€ìˆ˜ ê°„ *"ê´€ë ¨ì´ ìˆìŒ(associated)"*ì´ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•¨.\
    ì¦‰, $b_1$ì„ $x$ì˜ $y$ì— ëŒ€í•œ ì¸ê³¼ì  ì˜í–¥(causal impact)ìœ¼ë¡œ í•´ì„í•˜ë©´ ì•ˆ ë¨.\
    ì´ë¥¼ ì£¼ì¥í•˜ë ¤ë©´ íŠ¹ì • ì¡°ê±´ì´ ì¶©ì¡±ë˜ì–´ì•¼ í•¨.

-   ë˜í•œ $x$ì˜ ë‹¨ìœ„(unit)ì— ë”°ë¼ $b_1$ì˜ í•´ì„ê³¼ í¬ê¸°(magnitude)ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.

-   $x$ì˜ ë‹¨ìœ„ê°€ ë¬´ì—‡ì¸ì§€ ëª…í™•íˆ í•´ì•¼ í•¨

------------------------------------------------------------------------

## OLS with `R`

-   OLSëŠ” `lm`í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì •ê°€ëŠ¥

```{r, echo = TRUE, eval = FALSE}
lm(formula = dependent variable ~  independent variable, data = data.frame containing the data)
```

***í•™ê¸‰ ê·œëª¨ì™€ í•™ìƒ ì„±ì ***

-   ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ëª¨í˜•ì„ OLSë¡œ ì¶”ì •í•˜ì: $\textrm{average math score}_i = b_0 + b_1 \textrm{class size}_i + e_i$

::::: columns
::: {.column width="50%"}
```{r echo=T, eval = FALSE}
# OLS regression of class size on average maths score
lm(avgmath_cs ~ classize, grades_avg_cs) 
```
:::

::: {.column width="50%"}
```{r echo=F, eval = TRUE}
# OLS regression of class size on average maths score
lm(avgmath_cs ~ classize, grades_avg_cs) 
```
:::
:::::

------------------------------------------------------------------------

## Ordinary Least Squares (OLS): Prediction

```{r echo=F, eval = TRUE}
# OLS regression of class size on average maths score
lm(formula = avgmath_cs~classize, data = grades_avg_cs)
```

ì´ ê²°ê³¼ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ ($_i$ ì²¨ì ìƒëµ):

$$
\begin{aligned}
\widehat y &= b_0 + b_1 x \\
\widehat {\text{average math score}} &= b_0 + b_1 \cdot \text{class size} \\
\widehat {\text{average math score}} &= 61.11 + 0.19 \cdot \text{class size}
\end{aligned}
$$

í•™ìƒ ìˆ˜ê°€ 26ëª…ì¼ ë•Œ ì˜ˆìƒë˜ëŠ” í‰ê·  ì„±ì ì€?

$$
\begin{aligned}
\widehat {\text{average math score}} &= 61.11 + 0.19 \cdot 26 \\
\widehat {\text{average math score}} &= 66.08
\end{aligned}
$$

------------------------------------------------------------------------

## Task 2 {background-color="#ffebf0"}

::: {style="position: absolute; top: -30px; right: 10px; font-size: 0.8em;"}
`r countdown(minutes = 5, top = 0)`
:::

ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ í•™ê¸‰ ê·œëª¨(class size) ìˆ˜ì¤€ì—ì„œ ì§‘ê³„:

```{r, eval = F}
grades_avg_cs <- grades %>%
  group_by(classize) %>%
  summarise(avgmath_cs = mean(avgmath),
            avgverb_cs = mean(avgverb))
```

1.  Rí‰ê·  ì–¸ì–´ ì ìˆ˜(ì¢…ì† ë³€ìˆ˜)ë¥¼ í•™ê¸‰ ê·œëª¨(ë…ë¦½ ë³€ìˆ˜)ì— ëŒ€í•´ íšŒê·€ë¶„ì„(regress) ìˆ˜í–‰. íšŒê·€ ê³„ìˆ˜(coefficients)ë¥¼ í•´ì„í•  ê²ƒ

2.  ì´ì „ íšŒê·€ë¶„ì„ì—ì„œ OLS ê³„ìˆ˜ $b_0$ ë° $b_1$ì„ ì§ì ‘ ê³„ì‚° (ê³µì‹ ì´ìš©). (íŒíŠ¸: `cov`, `var`, `mean` í•¨ìˆ˜ ì‚¬ìš©.)

3.  í•™ê¸‰ ê·œëª¨ê°€ 0ì¼ ë•Œ, ì˜ˆì¸¡ëœ í‰ê·  ì–¸ì–´ ì ìˆ˜ëŠ” ì–¼ë§ˆì¸ê°€? (ì´ ê°’ì´ ì‹¤ì œë¡œ ì˜ë¯¸ê°€ ìˆëŠ”ê°€?

4.  í•™ê¸‰ ê·œëª¨ê°€ 30ëª…ì¼ ë•Œ, ì˜ˆì¸¡ëœ í‰ê·  ì–¸ì–´ ì ìˆ˜ëŠ” ì–¼ë§ˆì¸ê°€?

------------------------------------------------------------------------

## ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨ì˜ ì„±ì§ˆ (Predictions and Residuals: Properties)

::::: columns
::: {.column width="50%"}
-   $\widehat{y}_i$ì˜ í‰ê· ì€ $\bar{y}$ì™€ ê°™ìŒ $$\begin{align} \frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\ &= b_0 + b_1  \bar{x}  = \bar{y} \end{align}$$

-   ì”ì°¨(residuals)ì˜ í‰ê· (ë˜ëŠ” í•©)ì€ 0. $$\begin{align} \frac{1}{N} \sum_{i=1}^N e_i &= \frac{1}{N} \sum_{i=1}^N (y_i - \widehat y_i) \\ &= \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i \\\ &= 0 \end{align}$$
:::

::: {.column width="50%"}
-   ì„¤ëª… ë³€ìˆ˜(regressor)ì™€ ì”ì°¨ëŠ” ì •ì˜ìƒ ì„œë¡œ ìƒê´€ì´ ì—†ìŒ.

    $$Cov(x_i, e_i) = 0$$

-   ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨ëŠ” ìƒê´€ì´ ì—†ìŒ.

    $$\begin{align} Cov(\widehat y_i, e_i) &= Cov(b_0 + b_1x_i, e_i) \\ &= b_1Cov(x_i,e_i) \\ &= 0 \end{align}$$

    ì´ëŠ” $Cov(a + bx, y) = bCov(x,y)$ë¼ëŠ” ì„±ì§ˆ ë•Œë¬¸.
:::
:::::

------------------------------------------------------------------------

## ì„ í˜•ì„± ê°€ì •: ë°ì´í„° ì‹œê°í™”ì˜ ì¤‘ìš”ì„±

-   **ê³µë¶„ì‚°(covariance)**, **ìƒê´€ê³„ìˆ˜(correlation)**, ê·¸ë¦¬ê³  **ë‹¨ìˆœ OLS íšŒê·€**ëŠ” ë‘ ë³€ìˆ˜ ê°„ **ì„ í˜• ê´€ê³„(linear relationships)**ë§Œ ì¸¡ì •í•œë‹¤ëŠ” ì ì„ ê¸°ì–µí•´ì•¼ í•¨.

-   ì„œë¡œ *ì™„ì „íˆ ë™ì¼í•œ* ìƒê´€ê³„ìˆ˜ ë° íšŒê·€ì„ ì„ ê°–ëŠ” ë‘ ê°œì˜ ë°ì´í„°ì…‹ì´ *ì™„ì „íˆ ë‹¤ë¥´ê²Œ* ë³´ì¼ ìˆ˜ë„ ìˆìŒ.

------------------------------------------------------------------------

## ì„ í˜•ì„± ê°€ì •: Anscombeì˜ ì˜ˆì œ

-   Francis Anscombe (1973)ëŠ” í†µê³„ì ìœ¼ë¡œ *ì™„ì „íˆ ë™ì¼í•œ* ë„¤ ê°œì˜ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì—ˆìŒ.\
    í•˜ì§€ë§Œ **ì‹œê°ì ìœ¼ë¡œ ë³´ë©´ ì™„ì „íˆ ë‹¤ë¦„!**

::::: columns
::: {.column width="60%"}
```{r, echo=FALSE, fig.height=4}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
covs = data.frame(dataset = 1:4, cov = 0.0)
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  covs[i,"cov"] = eval(parse(text = paste0("cov(anscombe$x",i,",anscombe$y",i,")")))
  covs[i,"var(y)"] = eval(parse(text = paste0("var(anscombe$y",i,")")))
  covs[i,"var(x)"] = eval(parse(text = paste0("var(anscombe$x",i,")")))
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 0, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "black", pch = 21, bg = "#d90502", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "#DE9854")
}
par(op)
```
:::

::: {.column width="40%"}
```{r,echo = FALSE}
ch = kable(round(covs,3))
ch %>%
   kable_styling(bootstrap_options = "striped", font_size = 20)
```
:::
:::::

------------------------------------------------------------------------

## ë°ì´í„°ì—ì„œ ë¹„ì„ í˜• ê´€ê³„? (Nonlinear Relationships in Data?)

::::: columns
::: {.column width="50%"}
-   íšŒê·€ ë¶„ì„ì—ì„œ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìŒ.

-   ë°©ë²•: *ê³ ì°¨í•­(higher order term)*ì„ ì¶”ê°€í•˜ë©´ ë¨.\
    $$
      y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
      $$

-   ì´ëŠ” **ë‹¤ì¤‘ íšŒê·€(multiple regression)**ì˜ í•œ í˜•íƒœì„.
:::

::: {.column width="50%"}
-   ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ì´ì „ íšŒê·€ ëª¨ë¸ì„ ì ìš©í•œë‹¤ê³  ê°€ì •í•¨:

```{r non-line-cars-ols2, echo=FALSE, fig.height=6}
data(mtcars)
mtcars %>% ggplot(aes(x = hp, y = mpg)) +
    geom_point() +
    stat_smooth(method='lm', formula = y~poly(x,2), se = FALSE, aes(colour="Nonlinear")) +
    stat_smooth(method='lm', se = FALSE, aes(colour="Linear")) +
    scale_colour_manual(name="legend", values=c("darkgreen", "#d90502")) +
    labs(x = "x",
         y = "y",
         title = "Nonlinear relationship between x and y") +
    theme_bw(base_size = 20) +
    theme(legend.position="top") +
    theme(legend.title = element_blank())
```
:::
:::::

------------------------------------------------------------------------

## ë¶„ì‚° ë¶„ì„ (Analysis of Variance)

-   ë‹¤ìŒ ê´€ê³„ë¥¼ ê¸°ì–µí•  ê²ƒ:\
    $$
    y_i = \widehat{y}_i + e_i
    $$

-   ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ë‹¤ìŒê³¼ ê°™ì€ ë¶„ì‚° ë¶„í•´(variance decomposition)**ë¥¼ ì–»ìŒ: $$\begin{align} 
    Var(y) &= Var(\widehat{y} + e)\\
    &= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y},e)\\
    &= Var(\widehat{y}) + Var(e)
    \end{align}$$

-   ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ:

    -   $Var(x+y) = Var(x) + Var(y) + 2Cov(x,y)$
    -   $Cov(\hat{y},e) = 0$

-   ì´ ë³€ë™ (SST) = ëª¨ë¸ì´ ì„¤ëª…í•œ ë³€ë™ (SSE) + ì„¤ëª…ë˜ì§€ ì•Šì€ ë³€ë™ (SSR)

------------------------------------------------------------------------

## ì í•©ë„ í‰ê°€ (Goodness of Fit)

-   $R^2$ ê°’ì€ **ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€(fit)** ì¸¡ì •í•˜ëŠ” ì§€í‘œ.

$$
R^2 = \frac{\text{variance explained}}{\text{total variance}} = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
$$

```         
* $R^2$ ê°’ì´ **1ì— ê°€ê¹Œìš¸ìˆ˜ë¡**, ëª¨ë¸ì˜ **ì„¤ëª…ë ¥(explanatory power)**ì´ ***ë§¤ìš° ë†’ìŒ***ì„ ì˜ë¯¸í•¨.

* $R^2$ ê°’ì´ **0ì— ê°€ê¹Œìš¸ìˆ˜ë¡**, ëª¨ë¸ì˜ **ì„¤ëª…ë ¥**ì´ ***ë§¤ìš° ë‚®ìŒ***ì„ ì˜ë¯¸í•¨.


* ì˜ˆë¥¼ ë“¤ì–´, $R^2 = 0.5$ì´ë©´, **$x$ì˜ ë³€í™”ê°€ $y$ì˜ ë³€í™” ì¤‘ 50%ë¥¼ ì„¤ëª…í•¨**ì„ ì˜ë¯¸í•¨.
```

-   ë‚®ì€ $R^2$ ê°’ì´ ë¬´ì¡°ê±´ ëª¨ë¸ì´ ì“¸ëª¨ì—†ë‹¤ëŠ” ëœ»ì€ ì•„ë‹˜! **ì˜ˆì¸¡(predictive power)**ë³´ë‹¤ëŠ” **ì¸ê³¼ì  ë©”ì»¤ë‹ˆì¦˜(causal mechanisms)**ì— ë” ì´ˆì ì„ ë§ì¶”ëŠ” ê²½ìš°ê°€ ë§ìŒ.

-   $R^2$ ê°’ì€ **ì¸ê³¼ ê´€ê³„(causal relationship)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œê°€ ì•„ë‹˜!** íšŒê·€ ëª¨ë¸ì—ì„œ ë†’ì€ $R^2$ ê°’ì´ ìˆë‹¤ê³  í•´ì„œ, $x$ê°€ $y$ë¥¼ ì¸ê³¼ì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤ê³  ë³¼ ìˆ˜ ì—†ìŒ!

------------------------------------------------------------------------

## Task 3 {background-color="#ffebf0"}

::: {style="position: absolute; top: -30px; right: 10px; font-size: 0.8em;"}
`r countdown(minutes = 10, top = 0)`
:::

1.  `avgmath_cs`ë¥¼ `classize`ì— ëŒ€í•´ íšŒê·€(regress)í•˜ê³  ê²°ê³¼ë¥¼ `math_reg` ê°ì²´ì— ì €ì¥.

2.  `summary(math_reg)`ë¥¼ ì‹¤í–‰í•˜ì—¬ **(ë‹¤ì¤‘)** $R^2$ ê°’ì„ í™•ì¸í•¨. ì´ ê°’ì˜ ì˜ë¯¸ë¥¼ í•´ì„í•  ê²ƒ.

3.  `classize`ì™€ `avgmath_cs` ê°„ **ìƒê´€ê³„ìˆ˜(correlation)**ë¥¼ ì œê³±í•˜ì—¬ ê³„ì‚°. ì´ ê°’ì€ **ë‹¨ì¼ ì„¤ëª…ë³€ìˆ˜**(one regressor)ë¥¼ ê°€ì§„ íšŒê·€ì—ì„œ $R^2$ì™€ ìƒê´€ê³„ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ë³´ì—¬ì¤Œ.

4.  1ë²ˆê³¼ 2ë²ˆì„ `avgverb_cs`ì— ëŒ€í•´ ë°˜ë³µí•¨.\
    ì–´ë–¤ ì‹œí—˜ì—ì„œ í•™ê¸‰ ê·œëª¨ì˜ ë¶„ì‚°ì´ í•™ìƒ ì ìˆ˜ì˜ ë¶„ì‚°ì„ ë” ë§ì´ ì„¤ëª…í•˜ëŠ”ì§€ ë¹„êµí•¨.

5.  (Optional) `broom` íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜ ë° ë¡œë“œí•œ í›„, `math_reg`ë¥¼ `augment()` í•¨ìˆ˜ì— ì „ë‹¬í•˜ì—¬ ìƒˆë¡œìš´ ê°ì²´ì— ì €ì¥í•¨.\
    `avgmath_cs`ì˜ ë¶„ì‚°(SST)ê³¼ ì˜ˆì¸¡ê°’ `.fitted`ì˜ ë¶„ì‚°(SSE)ì„ ì‚¬ìš©í•˜ì—¬ $R^2$ ê°’ì„ ì§ì ‘ ê³„ì‚°í•¨. (ì´ì „ ìŠ¬ë¼ì´ë“œì˜ ê³µì‹ì„ ì°¸ê³ í•  ê²ƒ.)

------------------------------------------------------------------------



## ğŸ” ì¸ê³¼ ê´€ê³„ë¥¼ ì°¾ì•„ê°€ëŠ” ê¸¸  

âœ… ***ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë‹¤ë£°ê¹Œ?*** :  ì½ê¸°(Read), ì •ë¦¬(Tidy), ì‹œê°í™”(Visualize)...

âœ… ***ë³€ìˆ˜ê°„ ê´€ê³„ë¥¼ ì–´ë–»ê²Œ ìš”ì•½í• ê¹Œ? *** ë‹¨ìˆœ ì„ í˜• íšŒê·€(Simple Linear Regression)

âŒ ì¸ê³¼ ê´€ê³„(Causality)ë€ ë¬´ì—‡ì¸ê°€?

âŒ ì „ì²´ ëª¨ì§‘ë‹¨ì„ ê´€ì¸¡í•˜ì§€ ëª»í•˜ë©´ ì–´ë–»ê²Œ í• ê¹Œ?

âŒ ìš°ë¦¬ì˜ ì—°êµ¬ ê²°ê³¼ê°€ ë‹¨ìˆœí•œ ë¬´ì‘ìœ„(Randomness) ë•Œë¬¸ì¼ ìˆ˜ë„ ìˆì„ê¹Œ?

âŒ ì‹¤ì œë¡œ ì™¸ìƒì„±ì„ ì–´ë–»ê²Œ ì°¾ì•„ë‚¼ ìˆ˜ ìˆì„ê¹Œ?


---


<link href="https://fonts.googleapis.com/css2?family=Pacifico&display=swap" rel="stylesheet">

::: {style="display: flex; justify-content: center; align-items: center; height: 70vh;"}
<h2 style="color: #ff6666; text-align: center; font-family: &#39;Pacifico&#39;, cursive; font-size: 50px;">

THE END!

</h2>
:::